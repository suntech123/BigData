# --Sqoop ships with a help tool. To display a list of all available tools, type the following command:

[cloudera@quickstart ~]$ sqoop help
usage: sqoop COMMAND [ARGS]

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  import-mainframe   Import datasets from a mainframe server to HDFS
  job                Work with saved jobs
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  merge              Merge results of incremental imports
  metastore          Run a standalone Sqoop metastore
  version            Display version information

See 'sqoop help COMMAND' for information on a specific command.

# -- You can display help for a specific tool by entering: sqoop help (tool-name); for example, sqoop help import.
#-- You can also add the --help argument to any command: sqoop import --help.

[cloudera@quickstart ~]$ sqoop help import
[cloudera@quickstart ~]$ sqoop import --help


# -- Using Command Aliases. We can use aliases created for different tools ( sqoop-import, sqoop-list-databases etc. )

[cloudera@quickstart ~]$ sqoop-list-databases \
> --connect $mysql \
> --username root \
> --password cloudera
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
16/07/11 21:03:30 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0
16/07/11 21:03:30 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/07/11 21:03:31 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
information_schema
cm
firehose
hue
metastore
mysql
nav
navms
oozie
retail_db
rman
sentry

# --Listing all the tables in a given database

[cloudera@quickstart ~]$ sqoop list-tables --connect jdbc:mysql://127.0.0.1:3306/retail_db --username root --password cloudera
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
16/07/11 21:32:14 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0
16/07/11 21:32:14 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/07/11 21:32:15 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
categories
customers
departments
order_items
orders
products

# -- Use the --table argument to select the table to import. For example, --table departments.
  -- This copies the data creating a directory with the name as table name in HDFS default directory (eg below command will create departments directory)

[cloudera@quickstart ~]$ hdfs dfs -ls 
Found 4 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test

[cloudera@quickstart ~]$ sqoop import \
> --connect jdbc:mysql://127.0.0.1:3306/retail_db \
> --username root \
> --password cloudera \
> --table departments

--or in single liner command

[cloudera@quickstart ~]$ sqoop list-tables --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments

Note1 : By default, sqoop will create a directory with the same name as the imported table inside your home directory on HDFS and import all data there. We can check a new directory with same name as imported table (i.e departments ) created in home directory.

Note2 : For any hdfs user the default directory is /user/<username>. As the user in CDH is cloudera so we have our default directory 
as /user/cloudera . We can check the directories under default HDFS directory for a user by (hdfs dfs -ls) command or more specifically (hdfs dfs -ls /user/cloudera).

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-24 22:16 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 /user/cloudera/demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-24 22:16 /user/cloudera/departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 /user/cloudera/sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 /user/cloudera/sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 /user/cloudera/test

[cloudera@quickstart ~]$ hadoop fs -ls departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-09 03:07 departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera         21 2016-07-09 03:07 departments/part-m-00000
-rw-r--r--   1 cloudera cloudera         10 2016-07-09 03:07 departments/part-m-00001
-rw-r--r--   1 cloudera cloudera          7 2016-07-09 03:07 departments/part-m-00002
-rw-r--r--   1 cloudera cloudera         22 2016-07-09 03:07 departments/part-m-00003

[cloudera@quickstart ~]$ hadoop fs -cat departments/part*
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

--- The default directory can be changed to any arbitrary directory on your HDFS using option --target-dir. But keep in mind that this directory must not exist prior to running the sqoop command.

--- This command will fail as /user/hdfs already exist.

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --target-dir /user/hdfs

................
.................
16/07/24 22:40:54 ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://quickstart.cloudera:8020/user/hdfs already exists
...............
..............

--- Because sun is not existing already hence below command will succeed.

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --target-dir /user/hdfs/sun

[cloudera@quickstart ~]$ hdfs dfs -ls /user/hdfs/
Found 2 items
drwxr-xr-x   - hdfs     supergroup          0 2016-07-05 21:08 /user/hdfs/.Trash
drwxr-xr-x   - cloudera supergroup          0 2016-07-24 22:46 /user/hdfs/sun

[cloudera@quickstart ~]$ hdfs dfs -ls /user/hdfs/sun
Found 5 items
-rw-r--r--   1 cloudera supergroup          0 2016-07-24 22:46 /user/hdfs/sun/_SUCCESS
-rw-r--r--   1 cloudera supergroup         21 2016-07-24 22:46 /user/hdfs/sun/part-m-00000
-rw-r--r--   1 cloudera supergroup         10 2016-07-24 22:46 /user/hdfs/sun/part-m-00001
-rw-r--r--   1 cloudera supergroup          7 2016-07-24 22:46 /user/hdfs/sun/part-m-00002
-rw-r--r--   1 cloudera supergroup         22 2016-07-24 22:46 /user/hdfs/sun/part-m-00003

[cloudera@quickstart ~]$ hdfs dfs -cat /user/hdfs/sun/part*
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

--- But changing target directory if we want to run multiple sqoop jobs for multiple tables. Alternative is the parameter 
    --warehouse-dir, which allows to specify only parent directory. It is similar to default option and if we try to run the same command again, it will throw error that /user/hdfs/departments already exists
    
[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --warehouse-dir /user/hdfs

[cloudera@quickstart ~]$ hdfs dfs -ls /user/hdfs/
Found 3 items
drwxr-xr-x   - hdfs     supergroup          0 2016-07-05 21:08 /user/hdfs/.Trash
drwxr-xr-x   - cloudera supergroup          0 2016-07-24 23:11 /user/hdfs/departments
drwxr-xr-x   - cloudera supergroup          0 2016-07-24 22:46 /user/hdfs/sun

# -- You can append a WHERE clause to this with the --where argument. For example: --where "id > 400".

[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --table categories \
> --where "category_id > 40"

[cloudera@quickstart ~]$ hdfs dfs -ls categories
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-11 22:51 categories/_SUCCESS
-rw-r--r--   1 cloudera cloudera         96 2016-07-11 22:51 categories/part-m-00000
-rw-r--r--   1 cloudera cloudera         66 2016-07-11 22:51 categories/part-m-00001
-rw-r--r--   1 cloudera cloudera         37 2016-07-11 22:51 categories/part-m-00002
-rw-r--r--   1 cloudera cloudera         89 2016-07-11 22:51 categories/part-m-00003

[cloudera@quickstart ~]$ hdfs dfs -cat categories/part*
41,6,Trade-In
42,7,Bike & Skate Shop
43,7,Camping & Hiking
44,7,Hunting & Shooting
45,7,Fishing
46,7,Indoor/Outdoor Games
47,7,Boating
48,7,Water Sports
49,8,MLB
50,8,NFL
51,8,NHL
52,8,NBA
53,8,NCAA
54,8,MLS
55,8,International Soccer
56,8,World Cup Shop
57,8,MLB Players
58,8,NFL Players

# -- You can select a subset of columns and control their ordering by using the --columns argument. 
  -- This should include a comma-delimited list of columns to import. For example: --columns "name,employee_id,jobtitle".
  
  [cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --table orders \
> --columns "order_customer_id,order_id,order_status"

[cloudera@quickstart ~]$ hadoop fs -ls orders
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-11 22:22 orders/_SUCCESS
-rw-r--r--   1 cloudera cloudera     362752 2016-07-11 22:22 orders/part-m-00000
-rw-r--r--   1 cloudera cloudera     374160 2016-07-11 22:22 orders/part-m-00001
-rw-r--r--   1 cloudera cloudera     373528 2016-07-11 22:22 orders/part-m-00002
-rw-r--r--   1 cloudera cloudera     374078 2016-07-11 22:22 orders/part-m-00003

[cloudera@quickstart ~]$ hadoop fs -cat orders/part*
7456,68869,PROCESSING
3343,68870,COMPLETE
4960,68871,PENDING
3354,68872,COMPLETE
4545,68873,PENDING
1601,68874,COMPLETE
10637,68875,ON_HOLD
4124,68876,COMPLETE
9692,68877,ON_HOLD
6753,68878,COMPLETE
778,68879,COMPLETE



# -- By default sqoop will use query select min(<split-by>), max(<split-by>) from <table name> to find out boundaries for creating splits. 
  -- In some cases this query is not the most optimal so you can specify any arbitrary query returning two numeric columns 
     using --boundary-query argument.

-----------need to specify example command here later.

# --> Free Form Query Import
  --> Instead of using the --table, --columns and --where arguments, you can specify a SQL statement with the --query argument.
  --> But While importing a free-form query, you must specify a destination directory with --target-dir.
  --> Your query must include the token $CONDITIONS which each Sqoop process will replace with a unique condition expression.
  --> You must also select a splitting column with --split-by.

***Error demonstartation
[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --query 'select * from categories' \
> --target-dir stuff

16/07/11 23:26:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0
16/07/11 23:26:21 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
When importing query results in parallel, you must specify --split-by.
Try --help for usage instructions.
  
**************************************************************************************************************************************
--- Using a file format other than CSV [ Avro format ]

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --as-avrodatafile

--similarly it creates a directory with same name as table name.

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 00:50 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test

--But if we check the file blocks created are as below(with .avro extention)

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 00:50 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera        406 2016-07-25 00:50 /user/cloudera/departments/part-m-00000.avro
-rw-r--r--   1 cloudera cloudera        394 2016-07-25 00:50 /user/cloudera/departments/part-m-00001.avro
-rw-r--r--   1 cloudera cloudera        391 2016-07-25 00:50 /user/cloudera/departments/part-m-00002.avro
-rw-r--r--   1 cloudera cloudera        407 2016-07-25 00:50 /user/cloudera/departments/part-m-00003.avro

[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/departments/part*
Objavro.schema�{"type":"record","name":"departments","doc":"Sqoop import of departments","fields":[{"name":"department_id","type":["null","int"],"default":null,"columnName":"department_id","sqlType":"4"},{"name":"department_name","type":["null","string"],"default":null,"columnName":"department_name","sqlType":"12"}],"tableName":"departments"}�[��`�3^��P���.FitnessFootwear�[��`�3^��P���Objavro.schema�{"type":"record","name":"departments","doc":"Sqoop import of departments","fields":[{"name":"department_id","type":["null","int"],"default":null,"columnName":"department_id","sqlType":"4"},{"name":"department_name","type":["null","string"],"default":null,"columnName":"department_name","sqlType":"12"}],"tableName":"departments"}�&�
��s�-ET�=Apparel�&�
��s�-ET�=Objavro.schema�{"type":"record","name":"departments","doc":"Sqoop import of departments","fields":[{"name":"department_id","type":["null","int"],"default":null,"columnName":"department_id","sqlType":"4"},{"name":"department_name","type":["null","string"],"default":null,"columnName":"department_name","sqlType":"12"}],"tableName":"departments"}���0���pB҃�
Golf���0���pB҃�Objavro.schema�{"type":"record","name":"departments","doc":"Sqoop import of departments","fields":[{"name":"department_id","type":["null","int"],"default":null,"columnName":"department_id","sqlType":"4"},{"name":"department_name","type":["null","string"],"default":null,"columnName":"department_name","sqlType":"12"}],"tableName":"departments"}���F�(�#)�g��9�/0

**************************************************************************************************************************************

--- Using file format other than CSV [ Sequence File ]

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --as-sequencefile

[cloudera@quickstart ~]$ hdfs dfs -ls 
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 01:04 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 01:04 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera        133 2016-07-25 01:04 /user/cloudera/departments/part-m-00000
-rw-r--r--   1 cloudera cloudera        102 2016-07-25 01:04 /user/cloudera/departments/part-m-00001
-rw-r--r--   1 cloudera cloudera         99 2016-07-25 01:04 /user/cloudera/departments/part-m-00002
-rw-r--r--   1 cloudera cloudera        134 2016-07-25 01:04 /user/cloudera/departments/part-m-00003

[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/departments/part*
SEQ!org.apache.hadoop.io.LongWritable
                                      departments����G�u���y����FitnessFootwearSEQ!org.apache.hadoop.io.LongWritable
                                                                                                                        departments�����jg��h�1
                                                                                                                                                ApparelSEQ!org.apache.hadoop.io.LongWritable
                                 departmentsv3��`$�V4���Q��jGolfSEQ!org.apache.hadoop.io.LongWritable
&#OutdoorsFan Shop[cloudera@quickstart ~]$                                                           departments�;k�y�<�^


**************************************************************************************************************************************

--- Compressing imported data ( Use parameter --compress )

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --compress

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 01:21 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments/
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 01:21 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera         41 2016-07-25 01:21 /user/cloudera/departments/part-m-00000.gz
-rw-r--r--   1 cloudera cloudera         30 2016-07-25 01:21 /user/cloudera/departments/part-m-00001.gz
-rw-r--r--   1 cloudera cloudera         27 2016-07-25 01:21 /user/cloudera/departments/part-m-00002.gz
-rw-r--r--   1 cloudera cloudera         42 2016-07-25 01:21 /user/cloudera/departments/part-m-00003.gz

Note: By default output files will be compressed using Gzip and files will end up with .gz extension. To choose any other compression method use parameter --compression-codec parameter. AS using below example uses BZip2 and files will end up with .bz2 extention.

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --compress --compression-codec org.apache.hadoop.io.compress.BZip2Codec

[cloudera@quickstart ~]$ hdfs dfs -ls 
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 01:43 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 01:43 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera         63 2016-07-25 01:43 /user/cloudera/departments/part-m-00000.bz2
-rw-r--r--   1 cloudera cloudera         55 2016-07-25 01:43 /user/cloudera/departments/part-m-00001.bz2
-rw-r--r--   1 cloudera cloudera         49 2016-07-25 01:43 /user/cloudera/departments/part-m-00002.bz2
-rw-r--r--   1 cloudera cloudera         69 2016-07-25 01:43 /user/cloudera/departments/part-m-00003.bz2

**************************************************************************************************************************************

---Speeding up import speed by using native utilities provided by the database.

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --direct

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 02:01 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 02:01 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera         21 2016-07-25 02:01 /user/cloudera/departments/part-m-00000
-rw-r--r--   1 cloudera cloudera         10 2016-07-25 02:01 /user/cloudera/departments/part-m-00001
-rw-r--r--   1 cloudera cloudera          7 2016-07-25 02:01 /user/cloudera/departments/part-m-00002
-rw-r--r--   1 cloudera cloudera         22 2016-07-25 02:01 /user/cloudera/departments/part-m-00003

Note: Using native utilities will greatly improve performance, as these are optimizrd to provide best possible transfer speed while putting less burden on the database server. But there are several limitation as below.

 1. Not all databases have available native utilities.
 2. SQOOP has direct support only for MySql and PostgreSql.
 3. We need to make sure that native utilities are available on all of your Hadoop TaskTracker nodes.
    eg both mysqldump and mysqlimport utilities must be installed on all nodes hosting a TaskTracker service.
 4. Another limitation not all parameters are supported. Native utilities usually support text output.
 5. Also parameters that customize the escape characters, type mapping, col and row delimiters, or the NULL substituition strings might not be supported in all the cases.
 
 
**************************************************************************************************************************************

--- Overriding Type Mapping  [ to provide examples after clearance ]


**************************************************************************************************************************************

--- Controlling Parallelism
 -- By default sqoop uses four concurrent map tasks to transfer data to Hadoop. Sometime tranfering bigger tables with more concurrent tasks should decrease the time required to transfer all data.
 
[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --table orders \
> --num-mappers 10

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 02:52 orders
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/orders
Found 11 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 02:52 /user/cloudera/orders/_SUCCESS
-rw-r--r--   1 cloudera cloudera     292973 2016-07-25 02:52 /user/cloudera/orders/part-m-00000
-rw-r--r--   1 cloudera cloudera     298045 2016-07-25 02:52 /user/cloudera/orders/part-m-00001
-rw-r--r--   1 cloudera cloudera     301249 2016-07-25 02:52 /user/cloudera/orders/part-m-00002
-rw-r--r--   1 cloudera cloudera     301346 2016-07-25 02:52 /user/cloudera/orders/part-m-00003
-rw-r--r--   1 cloudera cloudera     301023 2016-07-25 02:52 /user/cloudera/orders/part-m-00004
-rw-r--r--   1 cloudera cloudera     300917 2016-07-25 02:52 /user/cloudera/orders/part-m-00005
-rw-r--r--   1 cloudera cloudera     301036 2016-07-25 02:52 /user/cloudera/orders/part-m-00006
-rw-r--r--   1 cloudera cloudera     300931 2016-07-25 02:52 /user/cloudera/orders/part-m-00007
-rw-r--r--   1 cloudera cloudera     301150 2016-07-25 02:52 /user/cloudera/orders/part-m-00008
-rw-r--r--   1 cloudera cloudera     301274 2016-07-25 02:52 /user/cloudera/orders/part-m-00009

**************************************************************************************************************************************

--- Encoding NULL values: By default Sqoop encodes database NULL values using the null string constant. But downstream processing   (Hive Queries, custom MapReducejob, or Pig Script) use a different constant for encoding missing values.

[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/information_schema" \
> --username root \
> --password cloudera \
> --table statistics

Note: The above command failed because there is no primary key in the table. In such a scenario, we need to specify --split-by parameter so that it can be used by Sqoop for spawning multiple MapR threads or otherwise read the table sequentially by using parameter '-m 1'

16/07/25 03:39:07 ERROR tool.ImportTool: Error during import: No primary key could be found for table statistics. Please specify one with --split-by or perform a sequential import with '-m 1'.

---Hence below command is used.

[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/information_schema" \
> --username root \
> --password cloudera \
> --table statistics \
> --split-by TABLE_CATALOG

[cloudera@quickstart ~]$ hdfs dfs -ls 
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 03:50 statistics
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/statistics
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 03:50 /user/cloudera/statistics/_SUCCESS
-rw-r--r--   1 cloudera cloudera      45178 2016-07-25 03:50 /user/cloudera/statistics/part-m-00000

[cloudera@quickstart ~]$ hdfs dfs -cat  /user/cloudera/statistics/part* | head -10
null,hue,auth_group,0,hue,PRIMARY,1,id,A,3,null,null,,BTREE,
null,hue,auth_group,0,hue,name,1,name,A,3,null,null,,BTREE,
null,hue,auth_group_permissions,0,hue,PRIMARY,1,id,A,0,null,null,,BTREE,
null,hue,auth_group_permissions,0,hue,group_id,1,group_id,A,0,null,null,,BTREE,
null,hue,auth_group_permissions,0,hue,group_id,2,permission_id,A,0,null,null,,BTREE,
null,hue,auth_group_permissions,1,hue,auth_group_permissions_5f412f9a,1,group_id,A,0,null,null,,BTREE,
null,hue,auth_group_permissions,1,hue,auth_group_permissions_83d7f98b,1,permission_id,A,0,null,null,,BTREE,
null,hue,auth_permission,0,hue,PRIMARY,1,id,A,225,null,null,,BTREE,
null,hue,auth_permission,0,hue,content_type_id,1,content_type_id,A,225,null,null,,BTREE,
null,hue,auth_permission,0,hue,content_type_id,2,codename,A,225,null,null,,BTREE,
cat: Unable to write to output stream.

NOTE: As we can see the database NULL values are imported as null string in HDFS. SUB_PART is a numeric column and TABLE_CATALOG is character column.

mysql> select TABLE_CATALOG,TABLE_SCHEMA,NON_UNIQUE,SUB_PART from statistics where table_name = 'auth_permission';
+---------------+--------------+------------+----------+
| TABLE_CATALOG | TABLE_SCHEMA | NON_UNIQUE | SUB_PART |
+---------------+--------------+------------+----------+
| NULL          | hue          |          0 |     NULL |
| NULL          | hue          |          0 |     NULL |
| NULL          | hue          |          0 |     NULL |
| NULL          | hue          |          1 |     NULL |
+---------------+--------------+------------+----------+
4 rows in set (0.00 sec)

[cloudera@quickstart ~]$ hdfs dfs -copyToLocal /user/cloudera/statistics/part-m-00000 /home/cloudera/
[cloudera@quickstart ~]$ ls -ltr part*
-rw-r--r-- 1 cloudera cloudera 45178 Jul 25 04:11 part-m-00000

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/information_schema" --username root --password cloudera --table statistics --null-string '\\N' --null-non-string '\\N' --split-by TABLE_CATALOG

---Without encoding NULL values

null,hue,auth_permission,0,hue,PRIMARY,1,id,A,225,null,null,,BTREE,
null,hue,auth_permission,0,hue,content_type_id,1,content_type_id,A,225,null,null,,BTREE,
null,hue,auth_permission,0,hue,content_type_id,2,codename,A,225,null,null,,BTREE,
null,hue,auth_permission,1,hue,auth_permission_37ef4eb4,1,content_type_id,A,225,null,null,,BTREE,

---After encoding NULL values

\N,hue,auth_permission,0,hue,PRIMARY,1,id,A,225,\N,\N,,BTREE,
\N,hue,auth_permission,0,hue,content_type_id,1,content_type_id,A,225,\N,\N,,BTREE,
\N,hue,auth_permission,0,hue,content_type_id,2,codename,A,225,\N,\N,,BTREE,
\N,hue,auth_permission,1,hue,auth_permission_37ef4eb4,1,content_type_id,A,225,\N,\N,,BTREE,

**************************************************************************************************************************************

--- Importing All your Tables.

[cloudera@quickstart ~]$ sqoop import-all-tables \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera

[cloudera@quickstart ~]$ sqoop import-all-tables \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --exclude-tables products,orders




  
  
  
  
  
  
  
  
  
  

