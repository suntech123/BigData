========================= sqoop-export ================================

# The export tool exports a set of files from HDFS back to an RDBMS.
# The target table must already exist in the database.
# The input files are read and parsed into a set of records according to the user-specified delimiters.
# The default operation is to transform these into a set of INSERT statements that inject the records into the database.
# In "update mode," Sqoop will generate UPDATE statements that replace existing records in the database.
# In "call mode" Sqoop will make a stored procedure call for each record.

--- Syntax

$ sqoop export (generic-args) (export-args)
$ sqoop-export (generic-args) (export-args)

1. The Hadoop generic arguments must preceed any export arguments.
2. The export arguments can be entered in any order with respect to one another.

--- We can use help command for listing out parms ( Below is formated list )

[cloudera@quickstart ~]$ sqoop help export

Common arguments:
   --connect <jdbc-uri>                         Specify JDBC connect string
   --connection-manager <class-name>            Specify connection manager class name
   --connection-param-file <properties-file>    Specify connection parameters file
   --driver <class-name>                        Manually specify JDBC driver class to use
   --hadoop-home <hdir>                         Override $HADOOP_MAPRED_HOME_ARG
   --hadoop-mapred-home <dir>                   Override $HADOOP_MAPRED_HOME_ARG
   --help                                       Print usage instructions

-P                                              Read password from console
   --password <password>                        Set authentication password
   --password-alias <password-alias>            Credential provider password alias
   --password-file <password-file>              Set authentication password file path
   --relaxed-isolation                          Use read-uncommitted isolation for imports
   --skip-dist-cache                            Skip copying jars to distributed cache
   --username <username>                        Set authentication username
   --verbose                                    Print more information while working

Export control arguments:
   --batch                      Indicates underlying statements to be executed in batch mode
   --call <arg>                 Populate the table using this stored procedure (one call per row)
   --clear-staging-table        Indicates that any data in staging table can be deleted
   --columns <col,col,col...>   Columns to export to table
   --direct                     Use direct export fast path
   --export-dir <dir>           HDFS source path for the export

-m,--num-mappers <n>            Use 'n' map tasks to export in parallel
   --mapreduce-job-name <name>  Set name for generated mapreduce job
   --staging-table <table-name> Intermediate staging table
   --table <table-name>         Table to populate 
   --update-key <key>           Update records by specified key column
   --update-mode <mode>         Specifies how updates are performed when new rows are found with non-matching keys in database
   
Input parsing arguments:
   --input-enclosed-by <char>               Sets a required field encloser
   --input-escaped-by <char>                Sets the input escape character
   --input-fields-terminated-by <char>      Sets the input field separator
   --input-lines-terminated-by <char>       Sets the input end-of-line char
   --input-optionally-enclosed-by <char>    Sets a field enclosing character
   
Output line formatting arguments:
   --enclosed-by <char>             Sets a required field enclosing character
   --escaped-by <char>              Sets the escape character
   --fields-terminated-by <char>    Sets the field separator character
   --lines-terminated-by <char>     Sets the end-of-line character
   --mysql-delimiters               Uses MySQL's default delimiter set: fields: ,  lines: \n  escaped-by: \ optionally-enclosed-by: '
   --optionally-enclosed-by <char>  Sets a field enclosing character
   
--- To use export command for a file terminated by comma.

[cloudera@quickstart ~]$ sqoop export --connect "jdbc:mysql://localhost:3306/export_demo" --username root --password cloudera --table item --export-dir "/user/cloudera/export_demo" --input-fields-terminated-by ',';

[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/export_demo/item.csv
1,pencil
2,pen
3,scale
4,notebook
5,marker
6,book
7,thermostat
8,headphone
9,colour
10,mouse


mysql> select * from item order by id;
+------+------------+
| id   | name       |
+------+------------+
|    1 | pencil     |
|    2 | pen        |
|    3 | scale      |
|    4 | notebook   |
|    5 | marker     |
|    6 | book       |
|    7 | thermostat |
|    8 | headphone  |
|    9 | colour     |
|   10 | mouse      |
+------+------------+
10 rows in set (0.00 sec)

--- To use export command for multiple files in HDFS dir.

[cloudera@quickstart data_files]$ hdfs dfs -ls /user/cloudera/sqoop_demo/
Found 2 items
-rw-r--r--   1 cloudera cloudera         47 2016-08-31 00:09 /user/cloudera/sqoop_demo/item1.csv
-rw-r--r--   1 cloudera cloudera         28 2016-08-31 00:09 /user/cloudera/sqoop_demo/item2.csv

[cloudera@quickstart data_files]$ hdfs dfs -cat /user/cloudera/sqoop_demo/item1.csv
1,pen
2,pencil
3,duster
4,eraser
5,whiteboard

[cloudera@quickstart data_files]$ hdfs dfs -cat /user/cloudera/sqoop_demo/item2.csv
6,roster
7,mouse
8,notebook















--- To update the existing table with records by using the below parameter.
    --update-key <key>           Update records by specified key column
    
[1] Existing data in the table item.

mysql> select * from item;
+----+------------+
| id | name       |
+----+------------+
|  1 | pen        |
|  2 | pencil     |
|  3 | duster     |
|  4 | eraser     |
|  5 | whiteboard |
|  6 | roster     |
|  7 | mouse      |
|  8 | notebook   |
+----+------------+
8 rows in set (0.00 sec)

[2] Data is updated in the files for some keys in the export HDFS directory.

[cloudera@quickstart data_files]$ hdfs dfs -cat /user/cloudera/sqoop_demo/item1.csv
1,pen
2,long pencil
3,duster
4,eraser
5,whiteboard
[cloudera@quickstart data_files]$ hdfs dfs -cat /user/cloudera/sqoop_demo/item2.csv
6,roster
7,optical mouse
8,notebook

[3] Running the sqoop export command with the update parameters.

[cloudera@quickstart data_files]$ sqoop export \
> --connect "jdbc:mysql://quickstart.cloudera:3306/sun_export_db" \
> --username root \
> --password cloudera \
> --table item \
> --update-key id \
> --export-dir /user/cloudera/sqoop_demo \
> --input-fields-terminated-by ',' \
> ;

[4] Validate the updated records in table item.

mysql> select * from item;
+----+---------------+
| id | name          |
+----+---------------+
|  1 | pen           |
|  2 | long pencil   |
|  3 | duster        |
|  4 | eraser        |
|  5 | whiteboard    |
|  6 | roster        |
|  7 | optical mouse |
|  8 | notebook      |
+----+---------------+
8 rows in set (0.00 sec)


--- To 

[1] Existing data in item table.

mysql> select * from item;
+----+---------------+
| id | name          |
+----+---------------+
|  1 | pen           |
|  2 | long pencil   |
|  3 | duster        |
|  4 | eraser        |
|  5 | whiteboard    |
|  6 | roster        |
|  7 | optical mouse |
|  8 | notebook      |
+----+---------------+
8 rows in set (0.00 sec)

[2] Data is updated as well inserted into input files.

[cloudera@quickstart data_files]$ hdfs dfs -cat /user/cloudera/sqoop_demo/item1.csv
1,pen
2,long pencil
3,duster
4,super eraser
5,whiteboard
[cloudera@quickstart data_files]$ hdfs dfs -cat /user/cloudera/sqoop_demo/item2.csv
6,roster
7,optical mouse
8,notebook
9,keyboard
10,desktop

Note: As we can see one record in item1.csv updated and 2 records added in item2.csv.

[3] Run the sqoop export with only --update-key parameter.

[cloudera@quickstart data_files]$ sqoop export \
> --connect "jdbc:mysql://quickstart.cloudera:3306/sun_export_db" \
> --username root \
> --password cloudera \
> --table item \
> --export-dir /user/cloudera/sqoop_demo \
> --input-fields-terminated-by ',' \
> --update-key id \
> ;

[4] Validate data after export.

mysql> select * from item;
+----+---------------+
| id | name          |
+----+---------------+
|  1 | pen           |
|  2 | long pencil   |
|  3 | duster        |
|  4 | super eraser  |
|  5 | whiteboard    |
|  6 | roster        |
|  7 | optical mouse |
|  8 | notebook      |
+----+---------------+
8 rows in set (0.00 sec)
 
Note: As we can observe only updated record is visible but no extra added record visible which as per our expectation should have been    inserted.

[5] Now placing the original data in the table from backup.

mysql> select * from item;
+----+---------------+
| id | name          |
+----+---------------+
|  1 | pen           |
|  2 | long pencil   |
|  3 | duster        |
|  4 | eraser        |
|  5 | whiteboard    |
|  6 | roster        |
|  7 | optical mouse |
|  8 | notebook      |
+----+---------------+
8 rows in set (0.00 sec)

[6] Running the sqoop job with added parameter.

--update-mode <mode>         Specifies how updates are performed when new rows are found with non-matching keys in database

Note: we are choosing <mode> as allowinsert.

[cloudera@quickstart data_files]$ sqoop export \
> --connect "jdbc:mysql://quickstart.cloudera:3306/sun_export_db" \
> --username root \
> --password cloudera \
> --table item \
> --export-dir /user/cloudera/sqoop_demo \
> --input-fields-terminated-by ',' \
> --update-key id \
> --update-mode allowinsert \
> ;

[7] Validating data in item table.

mysql> select * from item;
+----+---------------+
| id | name          |
+----+---------------+
|  1 | pen           |
|  2 | long pencil   |
|  3 | duster        |
|  4 | super eraser  |
|  5 | whiteboard    |
|  6 | roster        |
|  7 | optical mouse |
|  8 | notebook      |
|  9 | keyboard      |
| 10 | desktop       |
+----+---------------+
10 rows in set (0.00 sec)

Note: Data is updated as well as inserted at the same time.

