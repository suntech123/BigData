=============== Sqoop Import ==========================

# -- Use the --table argument to select the table to import. For example, --table departments.
  -- This copies the data creating a directory with the name as table name in HDFS default directory (eg below command will create departments directory)

[cloudera@quickstart ~]$ hdfs dfs -ls 
Found 4 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test

[cloudera@quickstart ~]$ sqoop import \
> --connect jdbc:mysql://127.0.0.1:3306/retail_db \
> --username root \
> --password cloudera \
> --table departments

--or in single liner command

[cloudera@quickstart ~]$ sqoop list-tables --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments

Note1 : By default, sqoop will create a directory with the same name as the imported table inside your home directory on HDFS and import all data there. We can check a new directory with same name as imported table (i.e departments ) created in home directory.

Note2 : For any hdfs user the default directory is /user/<username>. As the user in CDH is cloudera so we have our default directory 
as /user/cloudera . We can check the directories under default HDFS directory for a user by (hdfs dfs -ls) command or more specifically (hdfs dfs -ls /user/cloudera).

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-24 22:16 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 /user/cloudera/demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-24 22:16 /user/cloudera/departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 /user/cloudera/sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 /user/cloudera/sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 /user/cloudera/test

[cloudera@quickstart ~]$ hadoop fs -ls departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-09 03:07 departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera         21 2016-07-09 03:07 departments/part-m-00000
-rw-r--r--   1 cloudera cloudera         10 2016-07-09 03:07 departments/part-m-00001
-rw-r--r--   1 cloudera cloudera          7 2016-07-09 03:07 departments/part-m-00002
-rw-r--r--   1 cloudera cloudera         22 2016-07-09 03:07 departments/part-m-00003

[cloudera@quickstart ~]$ hadoop fs -cat departments/part*
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

--- The default directory can be changed to any arbitrary directory on your HDFS using option --target-dir. But keep in mind that this directory must not exist prior to running the sqoop command.

--- This command will fail as /user/hdfs already exist.

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --target-dir /user/hdfs

................
.................
16/07/24 22:40:54 ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://quickstart.cloudera:8020/user/hdfs already exists
...............
..............

--- Because sun is not existing already hence below command will succeed.

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --target-dir /user/hdfs/sun

[cloudera@quickstart ~]$ hdfs dfs -ls /user/hdfs/
Found 2 items
drwxr-xr-x   - hdfs     supergroup          0 2016-07-05 21:08 /user/hdfs/.Trash
drwxr-xr-x   - cloudera supergroup          0 2016-07-24 22:46 /user/hdfs/sun

[cloudera@quickstart ~]$ hdfs dfs -ls /user/hdfs/sun
Found 5 items
-rw-r--r--   1 cloudera supergroup          0 2016-07-24 22:46 /user/hdfs/sun/_SUCCESS
-rw-r--r--   1 cloudera supergroup         21 2016-07-24 22:46 /user/hdfs/sun/part-m-00000
-rw-r--r--   1 cloudera supergroup         10 2016-07-24 22:46 /user/hdfs/sun/part-m-00001
-rw-r--r--   1 cloudera supergroup          7 2016-07-24 22:46 /user/hdfs/sun/part-m-00002
-rw-r--r--   1 cloudera supergroup         22 2016-07-24 22:46 /user/hdfs/sun/part-m-00003

[cloudera@quickstart ~]$ hdfs dfs -cat /user/hdfs/sun/part*
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

--- But changing target directory if we want to run multiple sqoop jobs for multiple tables. Alternative is the parameter 
    --warehouse-dir, which allows to specify only parent directory. It is similar to default option and if we try to run the same command again, it will throw error that /user/hdfs/departments already exists
    
[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --warehouse-dir /user/hdfs

[cloudera@quickstart ~]$ hdfs dfs -ls /user/hdfs/
Found 3 items
drwxr-xr-x   - hdfs     supergroup          0 2016-07-05 21:08 /user/hdfs/.Trash
drwxr-xr-x   - cloudera supergroup          0 2016-07-24 23:11 /user/hdfs/departments
drwxr-xr-x   - cloudera supergroup          0 2016-07-24 22:46 /user/hdfs/sun

# -- You can append a WHERE clause to this with the --where argument. For example: --where "id > 400".

[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --table categories \
> --where "category_id > 40"

[cloudera@quickstart ~]$ hdfs dfs -ls categories
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-11 22:51 categories/_SUCCESS
-rw-r--r--   1 cloudera cloudera         96 2016-07-11 22:51 categories/part-m-00000
-rw-r--r--   1 cloudera cloudera         66 2016-07-11 22:51 categories/part-m-00001
-rw-r--r--   1 cloudera cloudera         37 2016-07-11 22:51 categories/part-m-00002
-rw-r--r--   1 cloudera cloudera         89 2016-07-11 22:51 categories/part-m-00003

[cloudera@quickstart ~]$ hdfs dfs -cat categories/part*
41,6,Trade-In
42,7,Bike & Skate Shop
43,7,Camping & Hiking
44,7,Hunting & Shooting
45,7,Fishing
46,7,Indoor/Outdoor Games
47,7,Boating
48,7,Water Sports
49,8,MLB
50,8,NFL
51,8,NHL
52,8,NBA
53,8,NCAA
54,8,MLS
55,8,International Soccer
56,8,World Cup Shop
57,8,MLB Players
58,8,NFL Players

# -- You can select a subset of columns and control their ordering by using the --columns argument. 
  -- This should include a comma-delimited list of columns to import. For example: --columns "name,employee_id,jobtitle".
  
  [cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --table orders \
> --columns "order_customer_id,order_id,order_status"

[cloudera@quickstart ~]$ hadoop fs -ls orders
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-11 22:22 orders/_SUCCESS
-rw-r--r--   1 cloudera cloudera     362752 2016-07-11 22:22 orders/part-m-00000
-rw-r--r--   1 cloudera cloudera     374160 2016-07-11 22:22 orders/part-m-00001
-rw-r--r--   1 cloudera cloudera     373528 2016-07-11 22:22 orders/part-m-00002
-rw-r--r--   1 cloudera cloudera     374078 2016-07-11 22:22 orders/part-m-00003

[cloudera@quickstart ~]$ hadoop fs -cat orders/part*
7456,68869,PROCESSING
3343,68870,COMPLETE
4960,68871,PENDING
3354,68872,COMPLETE
4545,68873,PENDING
1601,68874,COMPLETE
10637,68875,ON_HOLD
4124,68876,COMPLETE
9692,68877,ON_HOLD
6753,68878,COMPLETE
778,68879,COMPLETE



# -- By default sqoop will use query select min(<split-by>), max(<split-by>) from <table name> to find out boundaries for creating splits. 
  -- In some cases this query is not the most optimal so you can specify any arbitrary query returning two numeric columns 
     using --boundary-query argument.

-----------need to specify example command here later.

# --> Free Form Query Import
  --> Instead of using the --table, --columns and --where arguments, you can specify a SQL statement with the --query argument.
  --> But While importing a free-form query, you must specify a destination directory with --target-dir.
  --> Your query must include the token $CONDITIONS which each Sqoop process will replace with a unique condition expression.
  --> You must also select a splitting column with --split-by.

***Error demonstartation
[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --query 'select * from categories' \
> --target-dir stuff

16/07/11 23:26:21 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0
16/07/11 23:26:21 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
When importing query results in parallel, you must specify --split-by.
Try --help for usage instructions.
  
**************************************************************************************************************************************
--- Using a file format other than CSV [ Avro format ]

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --as-avrodatafile

--similarly it creates a directory with same name as table name.

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 00:50 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test

--But if we check the file blocks created are as below(with .avro extention)

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 00:50 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera        406 2016-07-25 00:50 /user/cloudera/departments/part-m-00000.avro
-rw-r--r--   1 cloudera cloudera        394 2016-07-25 00:50 /user/cloudera/departments/part-m-00001.avro
-rw-r--r--   1 cloudera cloudera        391 2016-07-25 00:50 /user/cloudera/departments/part-m-00002.avro
-rw-r--r--   1 cloudera cloudera        407 2016-07-25 00:50 /user/cloudera/departments/part-m-00003.avro

[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/departments/part*
Objavro.schema�{"type":"record","name":"departments","doc":"Sqoop import of departments","fields":[{"name":"department_id","type":["null","int"],"default":null,"columnName":"department_id","sqlType":"4"},{"name":"department_name","type":["null","string"],"default":null,"columnName":"department_name","sqlType":"12"}],"tableName":"departments"}�[��`�3^��P���.FitnessFootwear�[��`�3^��P���Objavro.schema�{"type":"record","name":"departments","doc":"Sqoop import of departments","fields":[{"name":"department_id","type":["null","int"],"default":null,"columnName":"department_id","sqlType":"4"},{"name":"department_name","type":["null","string"],"default":null,"columnName":"department_name","sqlType":"12"}],"tableName":"departments"}�&�
��s�-ET�=Apparel�&�
��s�-ET�=Objavro.schema�{"type":"record","name":"departments","doc":"Sqoop import of departments","fields":[{"name":"department_id","type":["null","int"],"default":null,"columnName":"department_id","sqlType":"4"},{"name":"department_name","type":["null","string"],"default":null,"columnName":"department_name","sqlType":"12"}],"tableName":"departments"}���0���pB҃�
Golf���0���pB҃�Objavro.schema�{"type":"record","name":"departments","doc":"Sqoop import of departments","fields":[{"name":"department_id","type":["null","int"],"default":null,"columnName":"department_id","sqlType":"4"},{"name":"department_name","type":["null","string"],"default":null,"columnName":"department_name","sqlType":"12"}],"tableName":"departments"}���F�(�#)�g��9�/0

**************************************************************************************************************************************

--- Using file format other than CSV [ Sequence File ]

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --as-sequencefile

[cloudera@quickstart ~]$ hdfs dfs -ls 
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 01:04 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 01:04 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera        133 2016-07-25 01:04 /user/cloudera/departments/part-m-00000
-rw-r--r--   1 cloudera cloudera        102 2016-07-25 01:04 /user/cloudera/departments/part-m-00001
-rw-r--r--   1 cloudera cloudera         99 2016-07-25 01:04 /user/cloudera/departments/part-m-00002
-rw-r--r--   1 cloudera cloudera        134 2016-07-25 01:04 /user/cloudera/departments/part-m-00003

[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/departments/part*
SEQ!org.apache.hadoop.io.LongWritable
                                      departments����G�u���y����FitnessFootwearSEQ!org.apache.hadoop.io.LongWritable
                                                                                                                        departments�����jg��h�1
                                                                                                                                                ApparelSEQ!org.apache.hadoop.io.LongWritable
                                 departmentsv3��`$�V4���Q��jGolfSEQ!org.apache.hadoop.io.LongWritable
&#OutdoorsFan Shop[cloudera@quickstart ~]$                                                           departments�;k�y�<�^


**************************************************************************************************************************************

--- Compressing imported data ( Use parameter --compress )

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --compress

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 01:21 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments/
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 01:21 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera         41 2016-07-25 01:21 /user/cloudera/departments/part-m-00000.gz
-rw-r--r--   1 cloudera cloudera         30 2016-07-25 01:21 /user/cloudera/departments/part-m-00001.gz
-rw-r--r--   1 cloudera cloudera         27 2016-07-25 01:21 /user/cloudera/departments/part-m-00002.gz
-rw-r--r--   1 cloudera cloudera         42 2016-07-25 01:21 /user/cloudera/departments/part-m-00003.gz

Note: By default output files will be compressed using Gzip and files will end up with .gz extension. To choose any other compression method use parameter --compression-codec parameter. AS using below example uses BZip2 and files will end up with .bz2 extention.

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --compress --compression-codec org.apache.hadoop.io.compress.BZip2Codec

[cloudera@quickstart ~]$ hdfs dfs -ls 
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 01:43 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 01:43 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera         63 2016-07-25 01:43 /user/cloudera/departments/part-m-00000.bz2
-rw-r--r--   1 cloudera cloudera         55 2016-07-25 01:43 /user/cloudera/departments/part-m-00001.bz2
-rw-r--r--   1 cloudera cloudera         49 2016-07-25 01:43 /user/cloudera/departments/part-m-00002.bz2
-rw-r--r--   1 cloudera cloudera         69 2016-07-25 01:43 /user/cloudera/departments/part-m-00003.bz2

**************************************************************************************************************************************

---Speeding up import speed by using native utilities provided by the database.

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/retail_db" --username root --password cloudera --table departments --direct

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 02:01 departments
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/departments
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 02:01 /user/cloudera/departments/_SUCCESS
-rw-r--r--   1 cloudera cloudera         21 2016-07-25 02:01 /user/cloudera/departments/part-m-00000
-rw-r--r--   1 cloudera cloudera         10 2016-07-25 02:01 /user/cloudera/departments/part-m-00001
-rw-r--r--   1 cloudera cloudera          7 2016-07-25 02:01 /user/cloudera/departments/part-m-00002
-rw-r--r--   1 cloudera cloudera         22 2016-07-25 02:01 /user/cloudera/departments/part-m-00003

Note: Using native utilities will greatly improve performance, as these are optimizrd to provide best possible transfer speed while putting less burden on the database server. But there are several limitation as below.

 1. Not all databases have available native utilities.
 2. SQOOP has direct support only for MySql and PostgreSql.
 3. We need to make sure that native utilities are available on all of your Hadoop TaskTracker nodes.
    eg both mysqldump and mysqlimport utilities must be installed on all nodes hosting a TaskTracker service.
 4. Another limitation not all parameters are supported. Native utilities usually support text output.
 5. Also parameters that customize the escape characters, type mapping, col and row delimiters, or the NULL substituition strings might not be supported in all the cases.
 
 
**************************************************************************************************************************************

--- Overriding Type Mapping  [ to provide examples after clearance ]


**************************************************************************************************************************************

--- Controlling Parallelism
 -- By default sqoop uses four concurrent map tasks to transfer data to Hadoop. Sometime tranfering bigger tables with more concurrent tasks should decrease the time required to transfer all data.
 
[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --table orders \
> --num-mappers 10

[cloudera@quickstart ~]$ hdfs dfs -ls
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 02:52 orders
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/orders
Found 11 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 02:52 /user/cloudera/orders/_SUCCESS
-rw-r--r--   1 cloudera cloudera     292973 2016-07-25 02:52 /user/cloudera/orders/part-m-00000
-rw-r--r--   1 cloudera cloudera     298045 2016-07-25 02:52 /user/cloudera/orders/part-m-00001
-rw-r--r--   1 cloudera cloudera     301249 2016-07-25 02:52 /user/cloudera/orders/part-m-00002
-rw-r--r--   1 cloudera cloudera     301346 2016-07-25 02:52 /user/cloudera/orders/part-m-00003
-rw-r--r--   1 cloudera cloudera     301023 2016-07-25 02:52 /user/cloudera/orders/part-m-00004
-rw-r--r--   1 cloudera cloudera     300917 2016-07-25 02:52 /user/cloudera/orders/part-m-00005
-rw-r--r--   1 cloudera cloudera     301036 2016-07-25 02:52 /user/cloudera/orders/part-m-00006
-rw-r--r--   1 cloudera cloudera     300931 2016-07-25 02:52 /user/cloudera/orders/part-m-00007
-rw-r--r--   1 cloudera cloudera     301150 2016-07-25 02:52 /user/cloudera/orders/part-m-00008
-rw-r--r--   1 cloudera cloudera     301274 2016-07-25 02:52 /user/cloudera/orders/part-m-00009

**************************************************************************************************************************************

--- Encoding NULL values: By default Sqoop encodes database NULL values using the null string constant. But downstream processing   (Hive Queries, custom MapReducejob, or Pig Script) use a different constant for encoding missing values.

[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/information_schema" \
> --username root \
> --password cloudera \
> --table statistics

Note: The above command failed because there is no primary key in the table. In such a scenario, we need to specify --split-by parameter so that it can be used by Sqoop for spawning multiple MapR threads or otherwise read the table sequentially by using parameter '-m 1'

16/07/25 03:39:07 ERROR tool.ImportTool: Error during import: No primary key could be found for table statistics. Please specify one with --split-by or perform a sequential import with '-m 1'.

---Hence below command is used.

[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://localhost:3306/information_schema" \
> --username root \
> --password cloudera \
> --table statistics \
> --split-by TABLE_CATALOG

[cloudera@quickstart ~]$ hdfs dfs -ls 
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-18 13:30 demo
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 10:11 sqoop_import
drwxr-xr-x   - cloudera cloudera          0 2016-07-25 03:50 statistics
-rw-r--r--   1 cloudera cloudera          0 2016-07-18 13:15 sun1
drwxr-xr-x   - cloudera cloudera          0 2016-07-12 09:48 test

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/statistics
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2016-07-25 03:50 /user/cloudera/statistics/_SUCCESS
-rw-r--r--   1 cloudera cloudera      45178 2016-07-25 03:50 /user/cloudera/statistics/part-m-00000

[cloudera@quickstart ~]$ hdfs dfs -cat  /user/cloudera/statistics/part* | head -10
null,hue,auth_group,0,hue,PRIMARY,1,id,A,3,null,null,,BTREE,
null,hue,auth_group,0,hue,name,1,name,A,3,null,null,,BTREE,
null,hue,auth_group_permissions,0,hue,PRIMARY,1,id,A,0,null,null,,BTREE,
null,hue,auth_group_permissions,0,hue,group_id,1,group_id,A,0,null,null,,BTREE,
null,hue,auth_group_permissions,0,hue,group_id,2,permission_id,A,0,null,null,,BTREE,
null,hue,auth_group_permissions,1,hue,auth_group_permissions_5f412f9a,1,group_id,A,0,null,null,,BTREE,
null,hue,auth_group_permissions,1,hue,auth_group_permissions_83d7f98b,1,permission_id,A,0,null,null,,BTREE,
null,hue,auth_permission,0,hue,PRIMARY,1,id,A,225,null,null,,BTREE,
null,hue,auth_permission,0,hue,content_type_id,1,content_type_id,A,225,null,null,,BTREE,
null,hue,auth_permission,0,hue,content_type_id,2,codename,A,225,null,null,,BTREE,
cat: Unable to write to output stream.

NOTE: As we can see the database NULL values are imported as null string in HDFS. SUB_PART is a numeric column and TABLE_CATALOG is character column.

mysql> select TABLE_CATALOG,TABLE_SCHEMA,NON_UNIQUE,SUB_PART from statistics where table_name = 'auth_permission';
+---------------+--------------+------------+----------+
| TABLE_CATALOG | TABLE_SCHEMA | NON_UNIQUE | SUB_PART |
+---------------+--------------+------------+----------+
| NULL          | hue          |          0 |     NULL |
| NULL          | hue          |          0 |     NULL |
| NULL          | hue          |          0 |     NULL |
| NULL          | hue          |          1 |     NULL |
+---------------+--------------+------------+----------+
4 rows in set (0.00 sec)

[cloudera@quickstart ~]$ hdfs dfs -copyToLocal /user/cloudera/statistics/part-m-00000 /home/cloudera/
[cloudera@quickstart ~]$ ls -ltr part*
-rw-r--r-- 1 cloudera cloudera 45178 Jul 25 04:11 part-m-00000

[cloudera@quickstart ~]$ sqoop import --connect "jdbc:mysql://localhost:3306/information_schema" --username root --password cloudera --table statistics --null-string '\\N' --null-non-string '\\N' --split-by TABLE_CATALOG

---Without encoding NULL values

null,hue,auth_permission,0,hue,PRIMARY,1,id,A,225,null,null,,BTREE,
null,hue,auth_permission,0,hue,content_type_id,1,content_type_id,A,225,null,null,,BTREE,
null,hue,auth_permission,0,hue,content_type_id,2,codename,A,225,null,null,,BTREE,
null,hue,auth_permission,1,hue,auth_permission_37ef4eb4,1,content_type_id,A,225,null,null,,BTREE,

---After encoding NULL values

\N,hue,auth_permission,0,hue,PRIMARY,1,id,A,225,\N,\N,,BTREE,
\N,hue,auth_permission,0,hue,content_type_id,1,content_type_id,A,225,\N,\N,,BTREE,
\N,hue,auth_permission,0,hue,content_type_id,2,codename,A,225,\N,\N,,BTREE,
\N,hue,auth_permission,1,hue,auth_permission_37ef4eb4,1,content_type_id,A,225,\N,\N,,BTREE,

**************************************************************************************************************************************

--- Importing All your Tables.

[cloudera@quickstart ~]$ sqoop import-all-tables \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera

[cloudera@quickstart ~]$ sqoop import-all-tables \
> --connect "jdbc:mysql://localhost:3306/retail_db" \
> --username root \
> --password cloudera \
> --exclude-tables products,orders


*************************************************************************************************************************************

--- Importing a table into HDFS - basic import

[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://quickstart.cloudera:3306/sun_export_db" \
> --username root \
> --password cloudera \
> --table item \
> -m 1 \
> --target-dir /user/cloudera/sqoop_import \
> ;

Note: The target directory specified MUST already existing otherwise error will be thrown.

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/
Found 2 items
drwxr-xr-x   - cloudera cloudera          0 2016-08-31 01:44 /user/cloudera/sqoop_demo
drwxr-xr-x   - cloudera cloudera          0 2016-08-31 19:12 /user/cloudera/sqoop_import
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/sqoop_import
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2016-08-31 19:12 /user/cloudera/sqoop_import/_SUCCESS
-rw-r--r--   1 cloudera cloudera        115 2016-08-31 19:12 /user/cloudera/sqoop_import/part-m-00000
[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/sqoop_import/part-m-00000
1,pen
2,long pencil
3,duster
4,super eraser
5,whiteboard
6,roster
7,optical mouse
8,notebook
9,keyboard
10,desktop

--- Executing imports with an options file for static information

[cloudera@quickstart ~]$ sqoop --options-file /home/cloudera/param_file.txt \
> -m 1 \
> --table item \
> --target-dir /user/cloudera/sqoop_import \
> ;


--- Import all rows of a table in mySQL, but specific columns of the table

[cloudera@quickstart ~]$ sqoop --options-file /home/cloudera/param_file.txt \
> -m 1 \
> --table item_order \
> --columns order_id,item_name,order_date,qty \
> --target-dir /user/cloudera/sqoop_import2 \
> ;

Note: Pay attention to the comma seperated column list.

--- Import all columns, filter rows using where clause.

[cloudera@quickstart ~]$ sqoop --options-file /home/cloudera/param_file.txt \
> -m 1 \
> --table item_order \
> --where 'order_id > 105 and item_id > 3' \
> --target-dir /user/cloudera/sqoop_import3 \
> ;

Note: Whenever a condition is provided in --where parameter it MUST be quoted( either single or double quotes will work) whether it is     on single column or on multiple columns.

--- In spite of changing directory each time we can specify --warehouse-dir parameter.

[cloudera@quickstart ~]$ sqoop --options-file /home/cloudera/param_file.txt \
> -m 1 \
> --table item \
> --warehouse-dir /user/cloudera/sqoop_import \
> ;

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/
Found 2 items
drwxr-xr-x   - cloudera cloudera          0 2016-08-31 01:44 /user/cloudera/sqoop_demo
drwxr-xr-x   - cloudera cloudera          0 2016-08-31 22:45 /user/cloudera/sqoop_import
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/sqoop_import
Found 1 items
drwxr-xr-x   - cloudera cloudera          0 2016-08-31 22:45 /user/cloudera/sqoop_import/item

[cloudera@quickstart ~]$ sqoop --options-file /home/cloudera/param_file.txt -m 1 --table item_order --columns order_id,item_name,order_date,qty --warehouse-dir /user/cloudera/sqoop_import ;

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/sqoop_import
Found 2 items
drwxr-xr-x   - cloudera cloudera          0 2016-08-31 22:45 /user/cloudera/sqoop_import/item
drwxr-xr-x   - cloudera cloudera          0 2016-08-31 22:55 /user/cloudera/sqoop_import/item_order

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/sqoop_import/item_order
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2016-08-31 22:55 /user/cloudera/sqoop_import/item_order/_SUCCESS
-rw-r--r--   1 cloudera cloudera        392 2016-08-31 22:55 /user/cloudera/sqoop_import/item_order/part-m-00000

[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/sqoop_import/item_order/part-m-00000
100,sed,2016-12-21 22:20:27.0,8
101,adipiscing,2016-04-08 15:29:05.0,2
102,imperdiet,2017-08-21 02:59:02.0,7
103,nec,2015-10-03 01:30:15.0,3
104,augue,2016-07-24 04:15:31.0,6
105,vestibulum,2017-05-16 15:55:01.0,9
106,Nam,2017-07-11 22:26:46.0,6
107,feugiat,2016-05-14 22:15:03.0,6
108,natoque,2016-10-04 07:55:39.0,1
109,erat,2016-12-23 20:05:11.0,6
110,pellentesque,2016-01-31 12:52:44.0,1

[cloudera@quickstart ~]$ sqoop --options-file /home/cloudera/param_file.txt -m 1 --table item_order --where 'order_id > 105' --warehouse-dir /user/cloudera/sqoop_import ;
...
...
ception: Output directory hdfs://quickstart.cloudera:8020/user/cloudera/sqoop_import/item_order already exists
16/08/31 23:01:27 ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://quickstart.cloudera:8020/user/cloudera/sqoop_import/item_order already exists

Note: In normal mode if we try to import same table with different conditions it will throw above error as in case of --target-dir option. If we need to support this activity we need to implement incremental import.

--- Import with a free form query without where clause

$ sqoop --options-file SqoopImportOptions.txt \
--query 'select EMP_NO,FIRST_NAME,LAST_NAME from employees where $CONDITIONS' \
-m 1 \
--target-dir /user/airawat/sqoop-mysql/employeeFrfrmQrySmpl2

Note: Case of the column needs to match that used to create table, or else the import fails.

--- Import with a free form query with where clause.

$ sqoop --options-file SqoopImportOptions.txt \
--query 'select EMP_NO,FIRST_NAME,LAST_NAME from employees where EMP_NO < 20000 AND
$CONDITIONS' \
-m 1 \
--target-dir /user/airawat/sqoop-mysql/employeeFrfrmQry1


##################################################################################################################################
##################################################################################################################################

--------------Importing data in text format and loading it in hive table with AVRO/Paraquet/ORC formats.


[cloudera@quickstart ~]$ sqoop import \
> --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
> --username root \
> --password cloudera \
> --table order_items \
> --fields-terminated-by , \
> --target-dir '/user/cloudera/demo3'

------------Error reading file in below mentioned external table if i use------------
> --escaped-by \\ \
> --enclosed-by '\"' \
-------------------------------------------------------------------------------------

[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/demo3
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2016-10-03 21:52 /user/cloudera/demo3/_SUCCESS
-rw-r--r--   1 cloudera cloudera    1303818 2016-10-03 21:52 /user/cloudera/demo3/part-m-00000
-rw-r--r--   1 cloudera cloudera    1343222 2016-10-03 21:52 /user/cloudera/demo3/part-m-00001
-rw-r--r--   1 cloudera cloudera    1371917 2016-10-03 21:52 /user/cloudera/demo3/part-m-00002
-rw-r--r--   1 cloudera cloudera    1389923 2016-10-03 21:52 /user/cloudera/demo3/part-m-00003

--- STEP 2. Create an external table

hive> CREATE EXTERNAL TABLE ORDER_ITEMS_EXT(
    > order_item_id             int,
    > order_item_order_id       int,
    > order_item_product_id     int,
    > order_item_quantity       tinyint,
    > order_item_subtotal       float,
    > order_item_product_price  float
    >  )
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY ','
    > STORED AS TEXTFILE
    > location '/user/cloudera/demo3';
OK
Time taken: 0.072 seconds


--- Step 3. Create the ORC table.

hive> CREATE TABLE ORDER_ITEMS_ORC(
    > order_item_id             int,
    > order_item_order_id       int,
    > order_item_product_id     int,
    > order_item_quantity       tinyint,
    > order_item_subtotal       float,
    > order_item_product_price  float
    >  )
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY ','
    > STORED AS ORC;
OK
Time taken: 0.196 seconds

--- Step 4. Insert the data from the external table to the Hive ORC table.

hive> INSERT OVERWRITE TABLE ORDER_ITEMS_ORC SELECT * FROM ORDER_ITEMS_EXT;

Query ID = cloudera_20161003230808_34a802fc-5cc1-4f5d-9181-ed6c56910d4b
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
.........
.........
Total MapReduce CPU Time Spent: 2 seconds 860 msec
OK
Time taken: 14.507 seconds


--- STEP 5. Verify that you imported the data into the ORC-formatted table correctly:

hive> select * from ORDER_ITEMS_ORC limit 10;
OK
1	1	957	1	299.98	299.98
2	2	1073	1	199.99	199.99
3	2	502	5	250.0	50.0
4	2	403	1	129.99	129.99
5	4	897	2	49.98	24.99
6	4	365	5	299.95	59.99
7	4	502	3	150.0	50.0
8	4	1014	4	199.92	49.98
9	5	957	1	299.98	299.98
10	5	365	5	299.95	59.99
Time taken: 0.073 seconds, Fetched: 10 row(s)

--- STEP 6. Verify the storage requirement for this ORC table in Hive.

hive> dfs -ls -h /user/hive/warehouse/retail_db.db/order_items_orc;
Found 1 items
-rwxrwxrwx   1 cloudera supergroup    725.5 K 2016-10-03 23:41 /user/hive/warehouse/retail_db.db/order_items_orc/000000_0

 Note: It is taking 725.5 kb of HDFS storage.
 
--- STEP 7. Lets create the table in AVRO format.

CREATE TABLE ORDER_ITEMS_AVRO(
order_item_id             int,
order_item_order_id       int,
order_item_product_id     int,
order_item_quantity       tinyint,
order_item_subtotal       float,
order_item_product_price  float
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS AVRO;

INSERT OVERWRITE TABLE ORDER_ITEMS_AVRO SELECT * FROM ORDER_ITEMS_EXT;

hive> dfs -ls -h /user/hive/warehouse/retail_db.db/order_items_avro;
Found 1 items
-rwxrwxrwx   1 cloudera supergroup      3.8 M 2016-10-03 23:58 /user/hive/warehouse/retail_db.db/order_items_avro/000000_0

hive> select * from order_items_avro limit 5;
OK
1	1	957	1	299.98	299.98
2	2	1073	1	199.99	199.99
3	2	502	5	250.0	50.0
4	2	403	1	129.99	129.99
5	4	897	2	49.98	24.99
Time taken: 0.082 seconds, Fetched: 5 row(s)


--- STEP 8. Let us create same table with PARAQUET file format.

CREATE TABLE ORDER_ITEMS_PAR(
order_item_id             int,
order_item_order_id       int,
order_item_product_id     int,
order_item_quantity       tinyint,
order_item_subtotal       float,
order_item_product_price  float
 )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS PARQUET;

INSERT OVERWRITE TABLE ORDER_ITEMS_PAR SELECT * FROM ORDER_ITEMS_EXT;

hive> dfs -ls -h /user/hive/warehouse/retail_db.db/order_items_par;
Found 1 items
-rwxrwxrwx   1 cloudera supergroup      1.7 M 2016-10-04 00:10 /user/hive/warehouse/retail_db.db/order_items_par/000000_0

hive> select * from order_items_par limit 10;
OK
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/jars/parquet-pig-bundle-1.5.0-cdh5.7.0.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]
........
.........
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [shaded.parquet.org.slf4j.helpers.NOPLoggerFactory]
1	1	957	1	299.98	299.98
2	2	1073	1	199.99	199.99
3	2	502	5	250.0	50.0
4	2	403	1	129.99	129.99
5	4	897	2	49.98	24.99
6	4	365	5	299.95	59.99
7	4	502	3	150.0	50.0
8	4	1014	4	199.92	49.98
9	5	957	1	299.98	299.98
10	5	365	5	299.95	59.99
Time taken: 0.077 seconds, Fetched: 10 row(s)

INSERT OVERWRITE TABLE ORDER_ITEMS_TEX SELECT * FROM ORDER_ITEMS_EXT;

hive> dfs -ls -h /user/hive/warehouse/retail_db.db/order_items_tex;
Found 1 items
-rwxrwxrwx   1 cloudera supergroup      5.2 M 2016-10-04 00:21 /user/hive/warehouse/retail_db.db/order_items_tex/000000_0


--- STEP 9. Let's create table with sequence format.

CREATE TABLE ORDER_ITEMS_SEQ(
order_item_id             int,
order_item_order_id       int,
order_item_product_id     int,
order_item_quantity       tinyint,
order_item_subtotal       float,
order_item_product_price  float
 )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS SEQUENCEFILE;

INSERT OVERWRITE TABLE ORDER_ITEMS_SEQ SELECT * FROM ORDER_ITEMS_EXT;

hive> dfs -ls -h /user/hive/warehouse/retail_db.db/order_items_seq;
Found 1 items
-rwxrwxrwx   1 cloudera supergroup      7.2 M 2016-10-04 01:07 /user/hive/warehouse/retail_db.db/order_items_seq/000000_0










  
  
  
  
  
  
  
  
  
  

