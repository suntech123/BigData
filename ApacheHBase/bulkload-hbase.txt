==================== importing data from csv to HBase table ============================

Introduction: There are three Ways to Load Data From HDFS to HBase


========================== Method 1 =============================== 

###### Using ImportTsv to load csv to HBase ########

STEP 1. Create an HBase table.
        hbase(main):013:0> create 'sun105','cf'
           
STEP 2. Create a csv file from python utility and put it in HDFS
        [cloudera@quickstart ~]$ pwd
        /home/cloudera
        [cloudera@quickstart ~]$ ./generate-csv-v-02.py
         
 Note : This utility takes Hive DDL as input and creates a pipe delimited file of 1M records in the same directory.
           
Sample File: 

[cloudera@quickstart ~]$ head -10 test_file_2017-11-26_03_58_02.csv
1493210111|hbka29vc|1688829951
1818812415|0ov996cfnx83jbxqr|1920069631
556268543|2ycgylv6sk74kvjnog3|1491760127
883061759|evmsvqzyt0|723774463
1446159359|ucul5jb0k7r5h8|1657146367
2020085247|n25i3v|1674833919
1162717695|j6s5l72adm3i|1860752383
1932067839|thg3ejb5ly8ox|1979920383
182569983|vqn3h7yqv3r8slz9|2023417855
1400496127|onbfcqgm7 cxy 0|1146941951

[cloudera@quickstart ~]$ hadoop fs -copyFromLocal test_file_2017-11-26_03_58_02.csv /data/sundata/
[cloudera@quickstart ~]$ hadoop fs -chmod 777 /data/sundata/sun102.csv

STEP 3. USING IMPORTTSV TO LOAD CSV TO HBASE

[cloudera@quickstart ~]$ hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="|" -Dimporttsv.columns=HBASE_ROW_KEY,cf:CUSTODIAN_NATURAL_KEY,cf:ACCOUNT_NATURAL_KEY sun105 /data/sundata/test_file_2017-11-26_03_58_02.csv

   IMPORTANT NOTE: 1. -Dimporttsv.separator -- Input data separator default is TAB
                   2. -Dimporttsv.columns -- Comma-separated column names are to be provided with each column name as either a column                                                family or a column family:column qualifier. Here HBASE_ROW_KEY will be loaded from first                                                  column data of CSV file and rest of the col values loaded in 
                                             cf:CUSTODIAN_NATURAL_KEY,cf:ACCOUNT_NATURAL_KEY as specified in comma                                                                    delimited list.
                                             

STEP 4. Check data in HBase

hbase(main):015:0> scan 'sun105',{LIMIT => 10}
ROW                              COLUMN+CELL                                                                                 
 1000001535                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=1468613119                    
 1000001535                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=as0zxbd4eb1lm               
 1000004607                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=985068031                     
 1000004607                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=ayvvpdqqorqphlo84ro         
 1000008703                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=1741409791                    
 1000008703                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=qw0b8iikle                  
 1000010751                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=85395967                      
 1000010751                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=4s3dm8rhfm                  
 1000014335                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=398445055                     
 1000014335                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=386e9g8j5w4z                
 1000017407                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=1590032895                    
 1000017407                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=yl9veal6mbeoqcqx            
 1000019455                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=955386367                     
 1000019455                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=d6rlasgcouw4cu              
 1000026623                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=1641864191                    
 1000026623                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=wpvsltq9bi2b71              
 1000027135                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=2059969023                    
 1000027135                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=brg2b45htr8f                
 1000033791                      column=cf:ACCOUNT_NATURAL_KEY, timestamp=1511699907927, value=2045510143                    
 1000033791                      column=cf:CUSTODIAN_NATURAL_KEY, timestamp=1511699907927, value=wj0x1h74                    
10 row(s) in 0.0160 seconds
--------------------------------------------------------------------------------------------------------------------
hbase(main):014:0> count 'sun105',100000
Current count: 100000, row: 121706495                                                                                        
Current count: 200000, row: 143434751                                                                                        
Current count: 300000, row: 1651447295                                                                                       
Current count: 400000, row: 1868164607                                                                                       
Current count: 500000, row: 2086737407                                                                                       
Current count: 600000, row: 370691583                                                                                        
Current count: 700000, row: 587981311                                                                                        
Current count: 800000, row: 805935615                                                                                        
889547 row(s) in 23.1280 seconds

=> 889547


============================= Method 2 ===================================

##### 
      
STEP 1. Create HBase Table
STEP 2. Generate the sample CSV file from python utility and upload to HDFS

STEP 3. Run the ImportTsv utility not to load data directly to HBase table but to generate multiple HFiles from the CSV file

[cloudera@quickstart ~]$ hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator="|" -Dimporttsv.columns=HBASE_ROW_KEY,cf:CUSTODIAN_NATURAL_KEY,cf:ACCOUNT_NATURAL_KEY -Dimporttsv.bulk.output=/data/sundata/sunfile1 sun105 /data/sundata/test_file_2017-11-26_03_58_02.csv

   IMPORTANT NOTE: Notice the Option -Dimporttsv.bulk.output=/data/sundata/sunfile1 
                   which is the output HDFS directory for HFILES and it should not be pre-existing.HFiles will be generated under
                   under sub-folders for each column family.
                   
 STEP 4. Next, use the completebulkload utility to bulk upload the HFiles into an HBase table. The completebulkload may be run in one of          two modes -
                    1. as an explicit class name or
                    2. via the driver. 
         Using the class name, the syntax is as follows:
         
         [cloudera@quickstart ~]$ hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /data/sundata/sunfile1 sun105
         
         OR
         
         Using Driver(prior that run [cloudera@quickstart ~]$ export HADOOP_CLASSPATH="$HADOOP_CLASSPATH:/usr/lib/hbase/lib/*")
         
[cloudera@quickstart ~]$ hadoop jar /usr/lib/hbase/lib/hbase-server-1.2.0-cdh5.10.0.jar completebulkload /data/sundata/sunfile2 sun105
         
 
Note: The HFiles are deleted after load. 
